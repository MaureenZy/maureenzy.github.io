{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Math 178, Winter 2025\n",
    "\n",
    "You are encouraged to work in groups of up to 3 total students, but each student should make a submission on Canvas. (It's fine for everyone in the group to submit the same link.)\n",
    "\n",
    "Put the full names of everyone in your group (even if you're working alone) here. This makes grading easier.\n",
    "\n",
    "**Names**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a practice of tree and forest models. We will continue using the same dataset from last week.\n",
    "\n",
    "* Dataset: `world_cup22.csv`\n",
    "* Goal: Can we use decision trees and random forests to predict the number of goals scored in World Cup 2022 matches?\n",
    "### Introduction\n",
    "\n",
    "In this lab, we will explore two powerful machine learning algorithms: Decision Trees and Random Forests. These algorithms are widely used for both classification and regression tasks due to their simplicity and interpretability.\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "A Decision Tree is a flowchart-like structure where each internal node represents a decision based on a feature, each branch represents the outcome of the decision, and each leaf node represents a class label or a continuous value. The paths from the root to the leaf represent classification rules.\n",
    "\n",
    "#### Random Forests\n",
    "\n",
    "A Random Forest is an ensemble method that builds multiple decision trees and merges them together to get a more accurate and stable prediction. It reduces overfitting by averaging the results of multiple trees, which are trained on different subsets of the data.\n",
    "\n",
    "In this lab, we will:\n",
    "\n",
    "1. Train a Decision Tree classifier on the `world_cup22.csv` dataset.\n",
    "2. Visualize the decision tree to understand the decision-making process.\n",
    "3. Train a Random Forest classifier and analyze the feature importances.\n",
    "4. Compare the performance and interpretability of Decision Trees and Random Forests.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the Data\n",
    "\n",
    "1. **Load the Dataset**:\n",
    "    - Ensure `world_cup22.csv` is in your working directory.\n",
    "    - Load the dataset into a DataFrame.\n",
    "\n",
    "    ```python\n",
    "    df = pd.read_csv(\"world_cup22.csv\")\n",
    "    ```\n",
    "\n",
    "2. **Inspect the Data**:\n",
    "    - Display the first five rows to understand its structure.\n",
    "\n",
    "    ```python\n",
    "    df.head()\n",
    "    ```\n",
    "\n",
    "3. **Split the Data**:\n",
    "    - Separate the features and the target variable (`number of goals`).\n",
    "\n",
    "    ```python\n",
    "    X = df.drop('number of goals', axis=1)\n",
    "    y = df['number of goals']\n",
    "    ```\n",
    "\n",
    "4. **Train-Test Split**:\n",
    "    - Split the data into training and testing sets.\n",
    "\n",
    "    ```python\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    ```\n",
    "\n",
    "By following these steps, you will have a clean and prepared dataset ready for training machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw The Decision Tree\n",
    "\n",
    "To draw the decision tree, follow these steps:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Import Required Libraries**:\n",
    "    Ensure you have imported the necessary libraries for plotting the decision tree.\n",
    "    ```python\n",
    "    from sklearn.tree import plot_tree\n",
    "    import matplotlib.pyplot as plt\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create and Train the Decision Tree Classifier**:\n",
    "    Create and train the `DecisionTreeClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Plot the Decision Tree**:\n",
    "    Use the `plot_tree` function to visualize the decision tree.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above is the logic tree built by the decision tree algorithm. To classify a new observation we start at the <i>root node</i> up top. If the observation satisfies the logic statement at the top we go left and are classified as a $0$, else we go right and are classified as $1$. The two <i>children</i> of the root node are known as <i>leaf nodes</i> or <i>terminal nodes</i> because they have no children of their own so we just predict the majority class contained in that node.\n",
    "\n",
    "This is essentially the decision rule we came up with (which is the objectively correct one by the way), so in this example the decision tree did well.\n",
    "\n",
    "If we look in the plot above we notice a number of different stats in each node:\n",
    "\n",
    "- `samples`: the number of samples in each node. In this case, it represents the number of matches considered at each decision point.\n",
    "- `gini`: the gini impurity of the node, more on this in a moment. It measures the impurity or the likelihood of misclassification at each node.\n",
    "- `value`: the breakdown of the number of samples of each target value in the node. For example, if a node has `value = [50, 30, 20, 10, 5, 1]`, it means there are 50 matches where the number of goals is 0, 30 matches where the number of goals is 1, 20 matches where the number of goals is 2, 10 matches where the number of goals is 3, 5 matches where the number of goals is 4, and 1 match where the number of goals is 5.\n",
    "- A decision rule: The rule that is used for the following split. Samples that would be evaluated as True for the rule go to the left child, samples that would be evaluated as False go to the right child.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write here\n",
    "\n",
    "Write down what you observed here. Is the decision tree able to identify the importance of the categorical variable?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the Most Important Features\n",
    "## Forests are an Ensemble of Trees\n",
    "\n",
    "The *random forest* model is created by building many different decision trees. These trees are made \"different\" through a variety of random perturbations (more on this later in the notebook). Random forests are thus an ensemble of decision tree models.\n",
    "\n",
    "We will demonstrate the advantages of this ensemble with the synthetic dataset we used in our decision tree classification notebook. In the following code, we will use the `RandomForestClassifier` to identify the most important features in our dataset. The classifier will be trained on the data, and then we will extract and plot the feature importances to see which features have the most influence on the target variable.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Import the RandomForestClassifier**:\n",
    "    ```python\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    ```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Create and Train the RandomForestClassifier**:\n",
    "    ```python\n",
    "    forest_clf = RandomForestClassifier(n_estimators= 1000 , max_depth=3)\n",
    "    forest_clf.fit(df.iloc[???], df.iloc[???])\n",
    "    ```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Find the Most Important Features**:\n",
    "    ```python\n",
    "    importances = forest_clf.??? \n",
    "    # Sort the indices of the importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Print the Feature Ranking**:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Final Step: Plot the Feature Importances**:\n",
    "\n",
    "    ```plt.figure(figsize=(15, 5))\n",
    "    plt.title(\"Feature importances\")\n",
    "    ???\n",
    "    plt.show()```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
